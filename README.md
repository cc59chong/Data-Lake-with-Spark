# Project: Data-Lake-with-Spark
## Introduction
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.<br>
Building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.<br>
Testing the database and ETL pipeline by running queries by the analytics team from Sparkify and compare the results with their expected results.
## Project Description
Using Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. <br>
Loading data from S3, process the data into analytics tables using Spark, and load them back into S3. <br>
Deploying this Spark process on a cluster using AWS.
